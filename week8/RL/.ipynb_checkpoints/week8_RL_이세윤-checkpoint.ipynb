{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Playing Atari with Deep Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#128522; AlphaGo를 개발한 DeepMind의 논문이며 **DQN**의 개념을 알 수 있는 논문! &#128522;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High-Dimensional Sensory Input(Vision이나 Speech)에 강화학습을 사용하여 Control Policy를 성공적으로 학습하는 Deep Learning Model을 선보였다.\n",
    "- CNN 모델을 사용하며, 변형된 Q-Learning을 사용하여 학습하였다.\n",
    "> &#128587;Q-Learning이란  \n",
    "   - Model-Free 강화학습 알고리즘\n",
    "   - 에이전트(agent)가 주어진 상태 (state) 에서 행동(action)을 취했을 경우 받을 수 있는 보상(reward)의 기댓값을 예측하는 Q 함수를 사용하여 최적의 정책(policy)을 학습하는 강화학습 기법\n",
    "   - Q는 현재 state에서 취한 reward에 대한 quality\n",
    "- 2600개가 넘는 다양한 게임을 학습시키는데 동일한 모델과 동일한 학습 알고리즘을 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝을 강화학습에 도입하려는 시도를 하였다.\n",
    "> 딥러닝이 발전함에 따라 고차원 데이터들을 추출하는 것이 가능해짐\n",
    "> 딥러닝에서는 이러한 고차원의 데이터들을 입력으로 사용하여 CNN, MLP 등을 통해 지도 및 비지도학습에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 딥러닝을 강화학습에 적용하는데에 문제점을 발견하게 되었다.\n",
    "1. 대부분의 DL은 직접 라벨링된 많은 양의 training data를 필요로 한다. 그러나 RL은 sparse, noisy, delayed한 reward signal라는 scalar 값을 통해 학습을 해야한다.\n",
    "> 딥러닝과 달리 강화학습에서는 어떤 행위를 하게 되면 trial and error를 통해 action에 대한 결과를 알기까지의 시간(delay)이 필요하다.\n",
    "2. DL 알고리즘에서 각 데이터들은 독립적이지만 RL에서는 상호연관성이 높다.\n",
    "> 현재 state의 action이 다음 state의 reward에 영향을 주기 때문이다.\n",
    "3. RL에서는 알고리즘이 새로운 behavior를 배울 때 마다 data의 분포가 변하게 되는데 이는 데이터 분포가 고정되어 있다는 DL의 가정과 충돌한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 이 논문에서는 **CNN**이 성공적인 Control Policy를 학습할 수 있음을 증명한다. CNN은 변형된 Q-Learning을 사용하며 optimizer로 SGD를 사용한다. 또 데이터간 상호연관성이 있다는 문제와 데이터 분포가 변화한다는 문제를 해결하기 위해 **Experience Replay Memory**를 사용하였다.\n",
    "> &#128587;Experience Replay Memory란  \n",
    "   - 고르게 분포된 데이터를 사용하기 위한 방법  \n",
    "   - 거대한 Experience Replay Memory를 만들고 데이터(Experience)를 저장한 후 랜덤하게 뽑아서 학습  \n",
    "   - 이를 통해 correlated가 적고 고르게 분포된 데이터로 학습이 가능  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오직 video input과 reward, terminal signal, 그리고 action들로만 학습을 진행하였다. 또한 다양한 게임들에 대해 동일한 Network Architecture와 Hyperparmeter를 사용하여 학습을 진행하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent가 Environment와 상호작용하는 task를 생각해보자. 이 때 각 time-step마다 agent는 할 수 있는 action 가운데 하나를 선택하게 된다. action이 전달되면 emulator는 내부 상태를 변경하고 게임 점수를 수정한다. 여기에서 agent는 게임 내부 상태를 알 수 없고 단지 **현재 상태의 이미지(raw pixel vector)와 점수의 변화(reward)만을 전달** 받게 된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "게임 점수는 현재 action 뿐 아니라 이전의 행동들에 의존하여 결정되므로 action에 대한 피드백은 수천회의 time-step 후에나 얻게 된다. 따라서 **MDP(Markov Decision Process)**에 강화학습을 적용한다.\n",
    "> &#128587;MDP란  \n",
    "   - 시간 t에서의 상태는 t−1에서의 상태에만 영향을 받는다는 개념 활용\n",
    "   - agent는 state에서 action을 수행. 그러면 environment는 다음 state와 그에 상응하는 reward를 agent에게 반환.\n",
    "   ![https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile3.uf.tistory.com%2Fimage%2F99B1D7485ACF62550E6AE5](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile3.uf.tistory.com%2Fimage%2F99B1D7485ACF62550E6AE5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rt**: 시간 t에서 discounted factor가 정의된 reward\n",
    "$$R_t=\\sum^T_{t'=t}{r^{t'-t}r_{t'}}$$\n",
    "- T: 게임이 종료되는 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q*(s,a)**: 일련의 state(s)를 파악한 후에 취한 action(a)을 통해 얻을 수 있는 기댓값의 최대값을 반환하는 최적의 행동-가치 함수(action-value function) \n",
    "$$Q^*(s,a) = \\max_\\pi \\mathbb E [ R_t | s_t = s, a_t = a, \\pi ]$$\n",
    "- pi: st에서 at를 mapping하는 정책 함수\n",
    "- 최적의 Q 함수는 벨만 방정식이라는 특성을 따름\n",
    "> &#128587;벨만 기대방정식(Bellman Expectation Equation)\n",
    "> 1. 정책과 가치를 고려한 방정식\n",
    "> 2. 어떤 정책에서 기대할 수 있는 미래 보상의 합을 의미\n",
    "> 3. value iteration에 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 가능한 action a'에 대해 sequence s'의 다음 time step에서 최적의 Q*(s,a) 값을 알 수 있다면, Optimal strategy는 $r+\\gamma Q^*(s',a')$의 기대값을 최대화하는 것이다.\n",
    "$$Q^*(s,a) = \\mathbb E_{s^\\prime \\sim \\mathcal E} [ r + \\gamma Q^* (s^\\prime, a^\\prime) ~|~ s,a ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 Q 함수를 평가하기 위해 벨만 방정식을 iterative update한다. Value Iteration 알고리즘을 매 i번째 iteration마다 아래의 procedure를 수행하게 된다.\n",
    "$$Q_{i+1} (s,a) = \\mathbb E [ r + \\gamma Q^* (s', a') ~|~ s,a ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDP에서 $Q_i \\to Q^* \\mbox{ as } i\\to\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 action-value 함수는 각 sequence마다 독립적으로 측정되므로 위의 방식은 impratical하다. 따라서 function approximator를 사용한다.\n",
    "$$Q(s,a;\\theta) \\simeq Q^*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로는 Linear Function에 근사하지만, Non-linear Function에 근사하는 경우도 있다. Neural Net function approximator로 weight $\\theta$를 사용하는 것을 **Q-network**라고 한다. Q-network는 각 iteration마다 바뀌는 Loss function을 최소화시키는 방향으로 학습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_i (\\theta_i) = \\mathbb E_{s,a\\sim \\rho(\\cdot)} [ (y_i - Q(s,a;w_i) )^2 ],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mbox{where, }y_i = \\mathbb E_{s^\\prime \\sim \\mathcal E} [r + \\gamma \\max_{a^\\prime} Q(s^\\prime, a^\\prime;w_{i-1}) ~|~ s, a ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y_i$: iteration i의 target value\n",
    "- $\\rho(s,a)$: behavior distribution,  \n",
    "sequence s에 대해 action a의 probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때 $\\theta_{i-1}$는 Loss function $L_i(\\theta_i)$를 optimize할 때 고정된다. 이를 freeze target Q-network이라고 한다. 이는 지도학습과 다르게 target 값이 $\\theta$ 값에 민감하게 영향을 받기 때문에 stable한 학습을 위해 고정하는 것이다.  \n",
    "\n",
    "결과적으로 아래와 같은 Loss의 Gradient를 얻게 된다.\n",
    "$$\\nabla_{\\theta_i}L_i(\\theta_i) = \\mathbb E_{s,a\\sim \\rho(\\cdot)} [r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1}) - Q(s,a;\\theta_i))\\nabla_{\\theta_i}Q(s,a;\\theta_i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning 알고리즘은 $\\mathcal E$와 별도로 작동하는 model-free 알고리즘이다. Behavior Policy와 Learning Policy를 별도로 두는 Off-Policy를 사용한다. $\\epsilon$의 확률로 random action을 선택하고, $1 - \\epsilon$의 확률로 $a = max_aQ(s,a,;\\theta)$인 greedy strategy를 따른다.\n",
    "> 일반적으로 강화학습 데이터들은 correlation이 높기 때문에 data간 correlation을 깨기 위해 일정한 확률로 random action을 선택하고, 남은 확률로는 greedy strategy를 따라 행동을 선택하게 된다.\n",
    "- 탐험이 부족한 greedy algorithm을 개선시킨 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Related Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TD-gammon**:\n",
    "    - Model-Free 구조  \n",
    "    - MLP with One Hidden Layer의 Network  \n",
    "    - GO나 체스에 적용시 실패하였다.\n",
    "  \n",
    "  \n",
    "- **Model-Free Reinforcement Learning**:\n",
    "    - Non-Linear Function Approximator나 Off-Policy Learning에 적용시키면 Q-network가 발산하였다.  \n",
    "    - 수렴을 위해 주로 Linear Function Approximators를 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Neural fitted Q-Learning (NFQ)**:  \n",
    "    - Q-Network의 Parameter를 갱신시키기 위해 RPROP 알고리즘을 사용하여 $L_i (\\theta_i) = \\mathbb E_{s,a\\sim \\rho(\\cdot)} [ (y_i - Q(s,a;w_i) )^2 ]$ 에서의 Loss Function을 최적화시켰다.  \n",
    "    - SGD를 사용해 iteration을 돌기 위해 필요한 계산양을 줄였고, 큰 데이터셋까지 학습시킬 수 있었다.\n",
    "    - 처음으로 deep autoencoder를 사용하여 task의 low dimensional representation을 학습하였다.  \n",
    "    - 시각 입력을 사용하여 real-world control task를 성공적으로 NFQ 알고리즘에 적용하였다.  \n",
    "    (이와 반대로 이 논문에서는 시각적 입력으로부터 직접적으로 철저한 강화학습을 적용시켜 Action-Value를 판별하는 것과 같은 특징을 학습하였다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Deep Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**목표**: RL 알고리즘을 Deep Neural Network와 연결하여 RGB Image들에 직접적으로 작동하고, Stochastic Gradient Updates를 사용하여 Traning Data를 효율적으로 처리하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep RL에서는 몇 가지 이슈를 처리해야한다.\n",
    ">1. Deep Learning은 데이터가 i.i.d.하다고 가정하지만 실제 RL input의 데이터는 sequential하고 highly correlated 되어있다.\n",
    ">2. Policy 변화에 따른 (이 경우는 w의 변화에 따른) Q-value의 변화량이 너무 크기 때문에 policy가 oscillate하기 쉽다.\n",
    ">3. 위와 같은 세팅에서는 reward와 Q-value의 값이 엄청나게 커질 수 있기 때문에 stable한 SGD 업데이트가 어려워진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 이 이슈는 앞에서도 언급했듯이 **Experience Replay**를 사용해서 해결한다.\n",
    "2. 게임 방식을 아주 조금만 바꾸어도 게임의 결과가 완전히 크게 바뀌기 때문에 발생하는 현상이다. 또한 지도학습과는 다르게 target 값이 parameter에 영향을 민감하게 받기 때문에 이것을 고정해주는 과정이 필요한데, 이것을 **Freeze target Q-network**으로 해결한다.\n",
    "3. Q-value 값이 얼마나 커질지 모르기 때문에 stable한 update가 힘들어지는 것을 방지하기 위해 **Clip reward or normalize network adaptively to sensible range**로 해결한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freeze target Q-network**  \n",
    ": update하는 동안 target을 계산하기 위해 사용하는 paramter를 고정시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clip reward or normalize network adaptively to sensible range**  \n",
    ": reward의 값을 [-1,0,1] 중에서 하나만 선택하도록 강제하는 아이디어이다. 즉, 내가 100점을 얻거나 10000점을 얻거나 항상 reward는 +1 이다. ‘highest’ score를 얻는 것은 불가능하지만, 이렇게 설정함으로써 조금 더 stable한 update가 가능해진다. 그리고 그와는 별개로 모든 게임에 적용가능한 DQN을 learning할 수 있다는 장점도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience Replay**  \n",
    ": agent의 experience를 각 time stamp마다 다음과 같은 튜플 형태로 메모리 $\\mathcal D = e_1, \\ldots, e_N$에 저장한 후 이를 다시 사용한다.\n",
    "$$e_t = (s_t, a_t, r_t, s_{t+1})$$\n",
    "Experience replay는 이렇게 experience $e_t$를 메모리 $\\mathcal D$에 저장해두었다가, 임의로 샘플링하여 mini-batch를 구성한 후 parameter $\\theta$를 mini-batch에 대해 backpropagation으로 update하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay가 끝나고 agent는 action을 $\\epsilon$-greedy policy를 사용해 선택하고 실행한다. 이는 action을 $\\epsilon$의 확률로 random하게 고르거나 1-$\\epsilon$의 확률로 $a = max_aQ(s,a,;\\theta)$로 greedy하게 선택하는 policy를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 논문에서는 function $\\phi$를 정의해서 모든 $s_t$의 길이를 고정한다.  \n",
    "Deep Q-Learning의 알고리즘은 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![http://sanghyukchun.github.io/images/post/90-3.png](http://sanghyukchun.github.io/images/post/90-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - replay memory D와 capacity를 N으로 초기화한다.   \n",
    "> - action-value function Q를 random weight로 초기화한다.   \n",
    "> - **episode를 1~M까지 반복**  \n",
    "    >> - sequence $s_1$을 t=1일때의 이미지 $x_1$으로 초기화하고, 전처리과정을 통해 $\\phi_1$ 을 구한다.   \n",
    "    >> - **t를 1~T까지 반복**  \n",
    "        >>> - $\\epsilon$-greedy 알고리즘을 따라 무작위 action 또는 이전에 최선의 결과를 냈던 action 중 하나를 $a_t$로 선택한다.   \n",
    "        >>> - emulator에서 action $a_t$를 수행하고, reward $r_t$와 다음 image $x_{t+1}$를 받는다.   \n",
    "        >>> - 현재의 State $s_t$, 현재의 Action $a_t$, 새로운 image인 $x_{t+1}$ 을 $s_{t+1}$로 저장하고, $s_{t+1}$에 대해 전처리하여 $\\phi_{t+1}$을 구한다.  \n",
    "        >>> - replay memory $\\mathcal D$에 현재의 상태를 전처리한 값 $(\\phi_t, a_t, r_t, \\phi_{t+1})$을 저장한다.   \n",
    "        >>> - $\\mathcal D$에 저장된 Sample들 중에서 minibatch의 개수만큼 random하게 뽑는다.   \n",
    "        >>> - 여기서 yj를 정의하는데, 전처리한 결과인 $\\phi_{j+1}$ 이 목표 지점에 도달하면 $r_j$로, 목표지점이 아니라면 $r_j + \\gamma max_{a'} Q(\\phi_{j+1}, a';\\theta)$로 설정한다.   \n",
    "        >>> - Equation 3을 따라 Loss Function을 정의하고 gradient descent를 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN의 장점**  \n",
    "1. each step의 Experience가 잠재적으로 많은 weight update에 재사용되므로, Experience를 weight update 한번만 사용하는 기존의 방법보다 훨씬 data efficiency 하다.\n",
    "2. 연속적인 sample들로부터 학습을 진행하는 것은 데이터들간의 high correlations 때문에 비효율적이다. 따라서 sample들을 $\\epsilon$-greedy 알고리즘을 통해 randomize하여 sample들의 high correlations를 break하여 update의 효율성을 높인다.\n",
    "3. 이 방법을 통해 parameter를 update하게 되면 다음 training을 위한 data sample을 어느 정도 determine할 수 있다. 예를 들어서 내가 지금 오른쪽으로 움직이는 쪽으로 action을 고른다면 다음 sample들은 내가 오른쪽에 있는 상태의 sample들이 dominate하게 나올 것이라고 예측할 수 있다. 따라서 이 방법을 통해 training을 위한 다음 데이터를 무작정 뽑는 것이 아니라 현재 action을 고려하여 효율적으로 뽑을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN에서는 replay memory 안에 마지막 N개의 exprience만을 저장하고, update를 하기 위해 uniformly random하게 D에서 추출한다. 이는 모든 과거 experience가 동일한 weight를 가진다는 점과 메모리의 한계때문에 모든 history를 저장하지 못한다는 점에서 한계가 있다. 따라서 Sophisticated Sampling 전략이 필요할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Preprocessing and Model Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**  \n",
    "Atari는 128 color palette의 210 * 160 픽셀 이미지로 구성되어 있다.  \n",
    "이를 직접 작업하는 것은 많은 계산량을 필요로 한다.  \n",
    "따라서 input 차원을 줄이는 전처리 과정을 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전처리 과정\n",
    "1. RGB로 표현된 이미지를 Gray-Scale로 변환\n",
    "2. 210 * 160 픽셀의 이미지를 110 * 84로 down-sampling\n",
    "3. GPU 사용을 위해 110 * 84 픽셀의 이미지를 84 * 84로 크롭하여 마지막 4개의 history를 stack으로 쌓고, 이를 입력에 대한 Q 함수 값을 구하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**  \n",
    "Q-Value를 구하는 2가지 방법  \n",
    "1. history와 action을 input으로 하고, output으로 history와 그 action에 대해 예측된 Q-value를 구하는 것\n",
    "    > input으로 들어온 action에 대해 seperate forward pass 연산을 진행해야 하고 action 수가 증가함에 따라 연산도 선형으로 증가하는 단점 존재, 따라서 아래의 방법 사용\n",
    "2. history만을 input으로 하여 output으로 각 action에 대해 예측된 Q-value를 구하는 것\n",
    "    > input으로 state를 사용하여 각 state에서 수행가능한 모든 action에 대한 Q-value를 계산하므로 더 효율적  \n",
    "    주어진 state에서 가능한 모든 action에 대한 Q-value 값을 single forward pass로 처리할 수 있다는 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN의 구조 정리**  \n",
    "![https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FIFHAH%2FbtqBQoE8xgz%2Fsfc6SWoKWqJN2QLpQfdqg1%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FIFHAH%2FbtqBQoE8xgz%2Fsfc6SWoKWqJN2QLpQfdqg1%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm과 Hyperparameter에 대한 setting**  \n",
    "1. Reward Structure:  \n",
    "- 게임에 따라 scale이 다 다르던 것을 양의 보상은 1, 음의 보상은 -1, 변화 없음은 0으로 수정하여 error derivatives의 스케일을 제한하고 모든 게임에 동일한 learning rate 적용\n",
    "2. RMSProp Algorithm과 $\\epsilon$-greedy Algorithm:  \n",
    "- Optimizer로 32 size의 minibatch를 RMSProp에 사용.  \n",
    "- Behavior Policy로는 처음부터 100만번째 프레임까지는 $\\epsilon$-greedy 알고리즘 사용하여 1에서 0.1까지 동일한 비율로 감소, 이후 0.1로 고정\n",
    "3. Frame Skipping Technique:  \n",
    "- agent가 모든 frame을 보고 action을 취하는 것이 아닌 K번째 프레임을 보고 action을 선택하게 하였고 마지막 행동은 skip된 frame에 반복 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Training and Stability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning과는 다르게 Reinforcement Learning은 학습 중에 agent와 progress를 정확히 측정하기 어렵다. 이 논문에서 Evaluation Metric은 수많은 게임을 통한 평균화된 reward를 통해 training 동안 주기적으로 계산하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FofqNU%2FbtqBQoynkGk%2F50FaYAWTzaZENFjXV73OXk%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FofqNU%2FbtqBQoynkGk%2F50FaYAWTzaZENFjXV73OXk%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림을 보면 앞의 두 그래프는 steady한 progress를 만들어내지 못하며 noisy하다는 것을 알 수 있다. 그러나 뒤의 두 policy에 대한 action-value를 예측하는 Q-function은 상당히 안정적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Visualizing the Value Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문의 그림을 통해 value function이 복잡한 일련의 event들에 대해 어떻게 발전해나가야 할지 학습할 수 있음을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3 Main Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일정한 step까지 0.05의 $\\epsilon$값을 갖는 $\\epsilon$-greedy 알고리즘을 사용하여 학습한 결과이다. 각각의 숫자는 reward를 의미하는 것으로 값이 클수록 좋은 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![http://sanghyukchun.github.io/images/post/90-4.png](http://sanghyukchun.github.io/images/post/90-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakout, Enduro, Pong은 심지어 사람보다도 좋은 것을 알 수 있고, Space Invaders를 제외하면 기존 모델들보다 best 뿐 아니라 avg 까지 뛰어난 것을 볼 수 있다. 논문에서는 Q\\*bert, Seaquest, Spae Invaders 등의 게임에서 사람의 performance에 한참 미치지 못하는 이유로 이 게임들은 전략이 엄청나게 긴 time scale로 필요하기 때문에 조금 더 challenge한 문제라고 주장하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL을 위한 새로운 딥러닝 모델을 소개하였다. 이 모델을 통해 **raw pixel**들만의 입력으로 2600가지가 넘은 Atari 게임을 위한 Control Policy를 학습하는 것이 가능함을 증명하였다. 또한 SGD에 **Experience Replay Memory**를 적용한 **변형된 Q-Learning**을 소개하였다. 이는 Architecture나 Hyperparameter 변화 없이도 7개 중 6개의 게임에서 SOTA를 달성했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#127775; 참고:   \n",
    "https://mangkyu.tistory.com/60  \n",
    "http://sanghyukchun.github.io/90/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
